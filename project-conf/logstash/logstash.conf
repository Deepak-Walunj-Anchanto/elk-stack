# Logstash configuration for PT (Parcel Tracking)
# Copy this file to your ELK server (e.g. ./logstash.conf) — ELK lives on a different server/project.
#
# On the ELK server you must:
# 1. Use this exact file (output must use data_stream => true, not index => "logstash-%{+YYYY.MM.dd}").
# 2. Create role logstash_writer_pt with index pattern "logs-*-pt" and privileges create_index, create_doc, index, create, write (see elasticsearch-role-user-logstash-writer.json).
# 3. Create user logstash_writer with that role; set LOGSTASH_WRITER_PASSWORD in .env and in docker-compose environment.
#
# Single group: "pt". Six services produce logs → six Elasticsearch data streams:
#   logs-django-pt, logs-celery-pt, logs-celerybeat-pt, logs-flower-pt, logs-nginx-pt, logs-react-pt
#
# Log format from Django/Celery: LEVEL|LOGGER_NAME|TIMESTAMP|MESSAGE
#
# =============================================================================
# FIELDS PRODUCED
# =============================================================================
#   - group: "pt" (from filebeat)
#   - service: django | celery | celerybeat | flower | nginx | react (from filebeat → which data stream)
#   - stream: High-level category (django, server, system, aws, network, app, unknown)
#   - app_module: When stream=app (auth, parcels, tenant, notification, reports, etc.)
#   - logger_name: Full logger (apps.parcels.views, django.request, etc.)
#   - log_level: DEBUG | INFO | WARNING | ERROR | CRITICAL
#   - log_message: The actual log message
#
# =============================================================================
# KIBANA
# =============================================================================
#   Data view: logs-*-pt (all PT services) or logs-django-pt, logs-celery-pt, etc.
#   Query: group:pt AND service:django AND app_module:parcels AND log_level:ERROR
# =============================================================================

input {
  beats {
    port => 5044
  }
}

filter {
    # Safety: never allow stream arrays
  if [stream] and [stream] =~ /^\[/ {
    mutate {
      replace => { "stream" => "%{[stream][0]}" }
    }
  }

  # PT group only (Filebeat sends group: "pt" and service: django | celery | ...)
  if [group] == "pt" {

    # Parse the log message format: LEVEL|LOGGER_NAME|TIMESTAMP|MESSAGE
    grok {
      match => {
        "message" => [
          "^%{WORD:log_level}\|%{DATA:logger_name}\|%{TIMESTAMP_ISO8601:log_timestamp}\.%{INT:log_ms}\|%{GREEDYDATA:log_message}$",
          "^%{WORD:log_level}\|%{DATA:logger_name}\|%{TIMESTAMP_ISO8601:log_timestamp}\|%{GREEDYDATA:log_message}$"
        ]
      }
      tag_on_failure => ["_grokparsefailure_pt"]
    }

    mutate {
      replace => {
        "[data_stream][type]"      => "logs"
        "[data_stream][namespace]" => "pt"
        "[data_stream][dataset]"   => "%{[service]}"
        "[event][module]"          => "pt"
      }
    }

    # Categorize logger_name into stream (and app_module for app stream)
    if [logger_name] {

      # ===== FRAMEWORK STREAMS =====
      if [logger_name] =~ /^django\./ {
        mutate { 
          replace => { 
            "stream" => "django" 
            } 
          }
      }
      else if [logger_name] == "werkzeug" {
        mutate { 
          replace => { 
            "stream" => "server" 
            } 
          }
      }
      else if [logger_name] =~ /^boto/ or [logger_name] == "s3transfer" {
        mutate { 
          replace => { 
            "stream" => "aws" 
            } 
          }
      }
      else if [logger_name] == "urllib3" {
        mutate { 
          replace => { 
            "stream" => "network" 
            } 
          }
      }
      else if [logger_name] == "asyncio" or [logger_name] == "PIL" or [logger_name] == "root" {
        mutate { 
          replace => { 
            "stream" => "system" 
            } 
          }
      }

      # ===== APPLICATION STREAM (66 loggers) =====
      else if [logger_name] =~ /^apps\.custom_auth/ or [logger_name] =~ /^apps\.onboarding/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "auth"
            } 
          }
      }
      else if [logger_name] =~ /^apps\.parcels/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "parcels" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.tenant_manager/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "tenant" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.notification/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "notification" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.manage_reports/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "reports" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.integrations/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "integrations" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.dashboard/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "dashboard" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.data_governance/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "data_governance" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.branded/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "branded" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.container/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "container" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.harmonization/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "harmonization" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.logs/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "logs" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.cron_tasks/ or [logger_name] =~ /^celery/ or [logger_name] =~ /\.tasks$/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "tasks" 
            } 
          }
      }
      else if [logger_name] =~ /^apps\.common/ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "common" 
            } 
          }
      }
      else if [logger_name] =~ /^config\./ {
        mutate { 
          replace => { 
            "stream" => "app"
            }
          add_field => { 
            "app_module" => "config" 
            } 
          }
      }
      else {
        mutate {
          replace => { "stream" => "unknown" }
          add_field => { "app_module" => "unknown" }
        }
      }
    } else {
      mutate {
        replace => { "stream" => "unknown" }
        replace => { "logger_name" => "unparsed" }
      }
    }

    # Normalize @timestamp for data streams (required). Use parsed time if grok succeeded, else keep Beats @timestamp.
    if "_grokparsefailure_pt" not in [tags] and [log_timestamp] {
      date {
        match  => ["log_timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss"]
        target => "@timestamp"
      }
    }
  }
}

output {
  # PT group: one Elasticsearch data stream per service (namespace pt, dataset = service name)
  # Data stream name will be: logs-django-pt (type-dataset-namespace)
  if [group] == "pt" and [service] {
    elasticsearch {
      hosts                  => ["elasticsearch:9200"]
      user                   => "logstash_writer_pt"
      password               => "${LOGSTASH_WRITER_PASSWORD}"
      manage_template        => false
      ecs_compatibility   => "v8"
      data_stream            => true
    }
    stdout { codec => rubydebug }
  }
  # # Uncomment below to debug: see events in Logstash stdout (docker logs logstash)
  # else {
  #   stdout { codec => rubydebug { metadata => true } }
  # }
}
